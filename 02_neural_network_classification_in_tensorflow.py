# -*- coding: utf-8 -*-
"""02_neural_network_classification_in_tensorflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mQvy2srGnh1w-XfXyPr2U5QxftpE7Gt2

# Introduction to neural network classification in TensorFlow

+ Binary Classification
+ Multiclass Classification
+ Multilabel Classification

## Creating data to view and fit
"""

from sklearn.datasets import make_circles

n_samples=1000
X,y=make_circles(n_samples,
                 noise=0.03,
                 random_state=42)

# BINARY CLASSIFICATION--
# Visualize
import pandas as pd
circles=pd.DataFrame({"X0":X[:,0], "X1":X[:,1], "labal":y})
circles

# Visualize
import matplotlib.pyplot as plt
plt.scatter(X[:,0],X[:,1], c=y, cmap=plt.cm.RdYlBu)

# Basic model

import tensorflow as tf

tf.random.set_seed(42)

model_1=tf.keras.Sequential([
                             tf.keras.layers.Dense(1)
])

model_1.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.SGD(),
    metrics=["accuracy"]
)
model_1.fit(X,y,epochs=5)

# Improving model 

import tensorflow as tf

tf.random.set_seed(42)

model_1=tf.keras.Sequential([
                             tf.keras.layers.Dense(1)
])

model_1.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.SGD(),
    metrics=["accuracy"]
)
model_1.fit(X,y,epochs=200, verbose=0)
model_1.evaluate(X,y)

# Adding layers
 
import tensorflow as tf

tf.random.set_seed(42)

model_2=tf.keras.Sequential([
                             tf.keras.layers.Dense(1),
                             tf.keras.layers.Dense(1)
])

model_2.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.SGD(),
    metrics=["accuracy"]
)
model_2.fit(X,y,epochs=200, verbose=0)
model_2.evaluate(X,y)

# Adding more dense neurons

# Improving model 

import tensorflow as tf

tf.random.set_seed(42)

model_3=tf.keras.Sequential([
                             tf.keras.layers.Dense(100),
                             tf.keras.layers.Dense(10),
                             tf.keras.layers.Dense(1)
])

model_3.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(),
    metrics=["accuracy"]
)
model_3.fit(X,y,epochs=100, verbose=0)
model_3.evaluate(X,y)

import numpy as np
def plot_decision_boundary(model, X, y):
  """
  Plots the decision boundary created by a model predicting on X.
  This function was inspired by two resources:
  """
  # Define the axis boundaries of the plot and create a meshgrid
  x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1
  y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1
  xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),
                       np.linspace(y_min, y_max, 100))
  
  # Create X value (we're going to make predictions on these)
  x_in = np.c_[xx.ravel(), yy.ravel()] # stack 2D arrays together

  # Make predictions
  y_pred = model.predict(x_in)

  # Check for multi-class
  if len(y_pred[0]) > 1:
    print("doing multiclass classification")
    # We have to reshape our prediction to get them ready for plotting
    y_pred = np.argmax(y_pred, axis=1).reshape(xx.shape)
  else:
    print("doing binary classification")
    y_pred = np.round(y_pred).reshape(xx.shape)
  
  # Plot the decision boundary
  plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)
  plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)
  plt.xlim(xx.min(), xx.max())
  plt.ylim(yy.min(), yy.max())

# Check out model_3  by visualizing

plot_decision_boundary(
    model=model_3,
    X=X,
    y=y
)

tf.random.set_seed(42)

model_4=tf.keras.Sequential([
                             tf.keras.layers.Dense(1,activation=tf.keras.activations.linear),
])

model_4.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(lr=0.001),
    metrics=["accuracy"]
)

history=model_4.fit(X,y,epochs=100)

# Chek out data

plt.scatter(X[:,0],X[:,1], c=y, cmap=plt.cm.RdYlBu)

# Check model_4's preds

plot_decision_boundary(
    model=model_4,
    X=X,
    y=y
)

"""## Building a Neural netweokr with non-linear activation function"""

tf.random.set_seed(42)

model_5=tf.keras.Sequential([
                             tf.keras.layers.Dense(1,activation=tf.keras.activations.relu),
])

model_5.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(lr=0.001),
    metrics=["accuracy"]
)

history=model_5.fit(X,y,epochs=100)

tf.random.set_seed(42)

model_5=tf.keras.Sequential([
                             tf.keras.layers.Dense(4,activation=tf.keras.activations.relu),
                             tf.keras.layers.Dense(4,activation=tf.keras.activations.relu),
                             tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid),
])

model_5.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(lr=0.001),
    metrics=["accuracy"]
)

history=model_5.fit(X,y,epochs=250)

plot_decision_boundary(
    model=model_5,
    X=X,
    y=y,
)

"""## Modeling activation functions on plots"""

## Sigmoid

A=tf.cast(tf.range(-10,10), tf.float32)
A,plt.plot(A),

def sigmoid(x):
  return 1/(1+tf.exp(-x))

sigmoid(A), plt.plot(sigmoid(A))

# Relu- Rectified linear unit
def relu(x):
  return tf.maximum(0,x)

relu(A), plt.plot(A), plt.plot(relu(A))

"""## Evaluating and imroving our classification model"""

# Split into train and test sets

X_train,y_train=X[:800],y[:800]
X_test,y_test=X[800:],y[800:]
X_train.shape,X_test.shape,y_train.shape,y_test.shape

tf.random.set_seed(42)

model_8=tf.keras.Sequential([
                             tf.keras.layers.Dense(4,activation=tf.keras.activations.relu),
                             tf.keras.layers.Dense(4,activation=tf.keras.activations.relu),
                             tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid),
])

model_8.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),
    metrics=["accuracy"]
)

history=model_8.fit(X,y,epochs=25)

# Evaluate the mdoel on test data
model_8.evaluate(X_test,y_test)

plt.figure(figsize=(12,6))
plt.subplot(1,2,1)
plt.title("Training Data")
plot_decision_boundary(
    model=model_8,
    X=X_train,
    y=y_train,
)

plt.subplot(1,2,1)
plt.title("Test Data")
plot_decision_boundary(
    model=model_8,
    X=X_test,
    y=y_test,
)

"""## Plot the loss (or training) curves"""

# Convert the history object to a DataFrame
pd.DataFrame(history.history), pd.DataFrame(history.history).plot()
plt.title("Model_8 Loss curve")

"""## Finding the optimal learning rate for our model"""

tf.random.set_seed(42)

model_9=tf.keras.Sequential([
                             tf.keras.layers.Dense(4,activation=tf.keras.activations.relu),
                             tf.keras.layers.Dense(4,activation=tf.keras.activations.relu),
                             tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid),
])

model_9.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),
    metrics=["accuracy"]
)

history=model_9.fit(X,y,epochs=25)

# Create a learning rate callback
lr_scheduler =tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 *10 **(epoch/20))

# Fit the model using the lr scheduler call back

history_9=model_9.fit(
    X_train,
    y_train,
    epochs=100,
    callbacks=[lr_scheduler]
)

pd.DataFrame(history_9.history).plot(figsize=(10,7), xlabel="epochs")

# Plot the learning rate versus the loss
lrs = 1e-4 * (10 ** (tf.range(100)/20))
plt.figure(figsize=(10, 7))
plt.semilogx(lrs, history_9.history["loss"])
plt.xlabel("Learning Rate")
plt.ylabel("Loss")
plt.title("Learning rate vs. Loss");

# Using ideal lr on the model

tf.random.set_seed(42)

model_10=tf.keras.Sequential([
                             tf.keras.layers.Dense(4,activation=tf.keras.activations.relu),
                             tf.keras.layers.Dense(4,activation=tf.keras.activations.relu),
                             tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid),
])

model_10.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.02),
    metrics=["accuracy"]
)

history=model_10.fit(X,y,epochs=25)


history_10=model_10.fit(
    X_train,
    y_train,
    epochs=20,
)

"""## Creating a confusion matrix"""

from sklearn.metrics import confusion_matrix

# Make predictions
y_preds=model_10.predict(X_test)
confusion_matrix(y_test,y_preds)

y_preds[:10]
## Our predictions have come out in prediction probability forms:

# Convert prediction probabilities to binary format
tf.round(y_preds)[:10]

# Create confusion matrix
confusion_matrix(y_test, tf.round(y_preds))

# Prettify confusion matrix
import itertools

figsize = (10, 10)

# Create the confusion matrix
cm = confusion_matrix(y_test, tf.round(y_preds))
cm_norm = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis] # normalize our confusion matrix
n_classes = cm.shape[0]

fig, ax = plt.subplots(figsize=figsize)

# Create a matrix plot
cax = ax.matshow(cm, cmap=plt.cm.Blues)
fig.colorbar(cax)

classes = False

if classes:
  labels = classes
else:
  labels = np.arange(cm.shape[0])

# Label the axes
ax.set(title="Confusion Matrix",
       xlabel="Predicted Label",
       ylabel="True Label",
       xticks=np.arange(n_classes),
       yticks=np.arange(n_classes),
       xticklabels=labels, 
       yticklabels=labels)


ax.xaxis.set_label_position("bottom")
ax.xaxis.tick_bottom()

ax.yaxis.label.set_size(20)
ax.xaxis.label.set_size(20)
ax.title.set_size(20)


threshold = (cm.max() + cm.min()) / 2.

# Plot the text on each cell
for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
  plt.text(j, i, f"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)",
           horizontalalignment="center",
           color="white" if cm[i, j] > threshold else "black",
           size=15)

"""# Working with larger example with Multiclass classification  """

import tensorflow as tf
from tensorflow.keras.datasets import fashion_mnist

# Data alredy sorted to training and test data sets
(train_data, train_labels), (test_data, test_labels)=fashion_mnist.load_data()

# Show the first training example
print(f"Training sample: \n{train_data[0]}\n")
print(f"Training label: \n{train_labels[0]}\n") #label=specific class in multiclass classification

# Checking the shapes
train_data[0].shape, train_labels[0].shape

# Plot a single sample
import matplotlib.pyplot as plt

plt.imshow(train_data[0])
print(f"Class: {train_labels[0]}")

plt.imshow(train_data[7])
print(f"Class: {train_labels[7]}")

# Index our training labels

class_names = ["T-shirt/top", "Trouser", "Pullover", "Dress", "Coat", "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]

# Plot multiple random images of random MNIST
import random

plt.figure(figsize=(7, 7))
for i in range(4):
  ax = plt.subplot(2, 2, i+1)
  rand_index = random.choice(range(len(train_data)))
  plt.imshow(train_data[rand_index], cmap=plt.cm.binary)
  plt.title(class_names[train_labels[rand_index]])
  plt.axis(False)

"""## Building a multiclass classification model

+ Input shape= 28 x 28
+ Output shape=10
+ loss func: Categorical Cross Entropy (Sparse Categorical Cross Entropy if not one-hot-encoded)
+ Output layer activation: Softmax
"""

# Not one hot encoded
tf.random.set_seed(42)


model_11=tf.keras.Sequential([
                              tf.keras.layers.Flatten(input_shape=(28,28)), 
                              tf.keras.layers.Dense(4, tf.keras.activations.relu),
                              tf.keras.layers.Dense(4, tf.keras.activations.relu),
                              tf.keras.layers.Dense(10, tf.keras.activations.softmax)
])
model_11.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(),
    metrics=["accuracy"]
)

non_norm_history=model_11.fit(
    x=train_data,
    y=train_labels,
    epochs=10,
    validation_data=(test_data, test_labels)
)

# One hot encoded
tf.random.set_seed(42)


model_11=tf.keras.Sequential([
                              tf.keras.layers.Flatten(input_shape=(28,28)), 
                              tf.keras.layers.Dense(4, tf.keras.activations.relu),
                              tf.keras.layers.Dense(4, tf.keras.activations.relu),
                              tf.keras.layers.Dense(10, tf.keras.activations.softmax)
])
model_11.compile(
    loss=tf.keras.losses.CategoricalCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(),
    metrics=["accuracy"]
)

non_norm_history=model_11.fit(
    x=train_data,
    y=tf.one_hot(train_labels, depth=10),
    epochs=10,
    validation_data=(test_data, tf.one_hot(test_labels, depth=10))
)

model_11.summary()

# Normalizing our data

train_data_norm=train_data/255.0
test_data_norm=test_data/255.0

train_data_norm.min(), train_data_norm.max()

# Using normalized data on neural networks, not one-hot-encoded

# One hot encoded
tf.random.set_seed(42)


model_11=tf.keras.Sequential([
                              tf.keras.layers.Flatten(input_shape=(28,28)), 
                              tf.keras.layers.Dense(4, tf.keras.activations.relu),
                              tf.keras.layers.Dense(4, tf.keras.activations.relu),
                              tf.keras.layers.Dense(10, tf.keras.activations.softmax)
])
model_11.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(),
    metrics=["accuracy"]
)

norm_history=model_11.fit(
    x=train_data_norm,
    y=train_labels,
    epochs=10,
    validation_data=(test_data_norm, test_labels)
)

# Plot non-normalized data loss curves

import pandas
pd.DataFrame(non_norm_history.history).plot(title="Non Normalized data")
pd.DataFrame(norm_history.history).plot(title="Non Normalized data")

"""## Finding the idead learning rate"""

# Using normalized data on neural networks, not one-hot-encoded

# One hot encoded
tf.random.set_seed(42)


model_13=tf.keras.Sequential([
                              tf.keras.layers.Flatten(input_shape=(28,28)), 
                              tf.keras.layers.Dense(4, tf.keras.activations.relu),
                              tf.keras.layers.Dense(4, tf.keras.activations.relu),
                              tf.keras.layers.Dense(10, tf.keras.activations.softmax)
])
model_13.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(),
    metrics=["accuracy"]
)

# Create learning rate callback
lr_scheduler=tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10**(epoch/20))
find_lr_history=model_13.fit(
    x=train_data_norm,
    y=train_labels,
    epochs=40,
    validation_data=(test_data_norm, test_labels),
    callbacks=[lr_scheduler]
)

# Plot learninf rate decay curve

lrs = 1e-3 * (10**(tf.range(40)/20))
plt.semilogx(lrs, find_lr_history.history["loss"])
plt.xlabel("Learning rate")
plt.ylabel("Loss")
plt.title("Finding the ideal learning rate")

# Use ideal learning_rate
tf.random.set_seed(42)

# Create model
model_14 = tf.keras.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(4, activation="relu"),
  tf.keras.layers.Dense(4, activation="relu"),
  tf.keras.layers.Dense(10, activation="softmax")                       
])

# Compile model
model_14.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),
                 optimizer=tf.keras.optimizers.Adam(lr=0.001),
                 metrics=["accuracy"])

# Fit the model
history_14 = model_14.fit(train_data_norm,
                          train_labels,
                          epochs=20,
                          validation_data=(test_data_norm, test_labels))

"""## Evaluating multiclass classification model"""

# Create a confusion matrix

def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15):
  # Create the confusion matrix
  cm = confusion_matrix(y_true, y_pred)
  cm_norm = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis] # normalize our confusion matrix
  n_classes = cm.shape[0]

  fig, ax = plt.subplots(figsize=figsize)
  # Create a matrix plot
  cax = ax.matshow(cm, cmap=plt.cm.Blues)
  fig.colorbar(cax)

  # Set labels to be classes 
  if classes:
    labels = classes
  else:
    labels = np.arange(cm.shape[0])

  # Label the axes
  ax.set(title="Confusion Matrix",
        xlabel="Predicted Label",
        ylabel="True Label",
        xticks=np.arange(n_classes),
        yticks=np.arange(n_classes),
        xticklabels=labels, 
        yticklabels=labels)

  # Set x-axis labels to bottom
  ax.xaxis.set_label_position("bottom")
  ax.xaxis.tick_bottom()

  # Adjust label size
  ax.yaxis.label.set_size(text_size)
  ax.xaxis.label.set_size(text_size)
  ax.title.set_size(text_size)

  # Set threshold for different colors
  threshold = (cm.max() + cm.min()) / 2.

  # Plot the text on each cell
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    plt.text(j, i, f"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)",
            horizontalalignment="center",
            color="white" if cm[i, j] > threshold else "black",
            size=text_size)

# Make predictions with our model
y_probs=model_14.predict(test_data_norm)

# First five preds
y_probs[:5]

y_probs[0], tf.argmax(y_probs[0])

class_names[tf.argmax(y_probs[0])]

# Convert all pred probabilities to integers

y_preds=y_probs.argmax(axis=1)

# View the first 10 prediction labels
y_preds[:10]

# Make a confusion matrix
from sklearn.metrics import confusion_matrix

confusion_matrix(y_true=test_labels,
                 y_pred=y_preds)

make_confusion_matrix(
    y_true=test_labels,
    y_pred=y_preds,
    classes=class_names,
    figsize=(15,15),
    text_size=10
)

import random

def plot_random_image(model, images, true_labels, classes):
  "Picks a random img, plots it abd lavels it with a truth and a pred label"

  i=random.randint(0,len(images))

  target_image=images[i]
  pred_probs=model.predict(target_image.reshape(1,28,28))
  pred_label=classes[pred_probs.argmax()]
  true_label=classes[true_labels[i]]

  plt.imshow(target_image, cmap=plt.cm.binary)

  if pred_label == true_label:
    color = "green"
  else:
    color = "red"
  
  plt.xlabel("Pred: {} {:2.0f}% (True: {})".format(pred_label,
                                                   100*tf.reduce_max(pred_probs),
                                                   true_label),
             color=color)

# Check a random img and its pred

plot_random_image(model=model_14,
                  images=test_data_norm,
                  true_labels=test_labels,
                  classes=class_names)

"""## What patterns is our model learning?"""

model_14.layers

# Extract a paticular layer
model_14.layers[1]

# Get the patterns of a layer in our network

weights, biases=model_14.layers[1].get_weights()

weights, weights.shape

model_14.summary()

"""## Bias Vector"""

# Bias and Biases shapes

biases, biases.shape

from tensorflow.keras.utils import plot_model

plot_model(model_14, show_shapes=True)